

\documentclass[12pt]{article}

% ------ DE BUNDEL ''ALGORITHMS'' ------
% Bestaat uit twee pakketten:
	\usepackage{algorithmic}			% Voor het typesetten van de pseudocode.
	\usepackage{algorithm}			% Zodat LaTeX een blok pseudocode als een figuur of tabel
									% behandelt (nummering, floating, List of Algorithms).

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}
% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

\usepackage{pgf,tikz}

\usepackage{hyperref}

\usepackage{listings}

\usepackage{enumerate}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
%\usepackage{graphicx}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

%\usepackage{minted}

%\usepackage{color}

% EXTRA (ALLOWED IN PLOS ?)


%\usepackage[table]{xcolor}
%
\usepackage{colortbl}
%\usepackage{natbib}
% Use doublespacing - comment out for single spacing
%\usepackage{setspace}
%\doublespacing

\usepackage{lineno}
\linenumbers

% Text layout
\topmargin 0.0cm
\oddsidemargin 0.5cm
\evensidemargin 0.5cm
\textwidth 16cm
\textheight 21cm

% Bold the 'Figure #' in the caption and separate it with a period
% Captions will be left justified
\usepackage[labelfont=bf,labelsep=period,justification=raggedright]{caption}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\bigCI}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}
\newcommand{\nbigCI}{\cancel{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}} 

% Use the PLoS provided bibtex style
\bibliographystyle{plos2009}
%\bibliographystyle{harvard}
%\bibliographystyle{apalike}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% sets
\newcommand{\B}{\mathbb{B}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}


% Leave date blank
\date{}

\pagestyle{myheadings}
%% ** EDIT HERE **


%% ** EDIT HERE **
%% PLEASE INCLUDE ALL MACROS BELOW

%% END MACROS SECTION

\newcommand{\df}[1]{{\color{red}#1}}

\newtheorem{thm}{Theorem}%[section]
\newtheorem{lem}{Lemma}%[section]
\newtheorem{lemma}{Lemma}%[section]
\newtheorem{rem}{Remark}
\newtheorem{corollary}{Corollary}[section]
\newtheorem{defin}{Definition}[section]
\newtheorem{example}{Example}[section]


%\definecolor{lightgray}{gray}{0.9}
\definecolor{kugray}{RGB}{224,224,224}



\title{}
\author{Daniela Bustos-Korts, Emilie Millet, Bart-Jan van Rossum, Martin Boer?, Fred van Eeuwijk, Willem Kruijer}

\begin{document}
	
	%
	% Title must be 150 characters or less
	\begin{flushleft}
		\today \\
		{\Large
			\textbf{Testing for QTL by environment interaction in multi-environment GWAS}
		}
		% Insert Author names, affiliations and corresponding author email.
		\\
		(in alphabetical order) Martin Boer$^{1}$, Daniela Bustos-Korts$^{1}$, Willem Kruijer$^{1, \ast}$, Marcos Malosetti$^{1}$, Emilie Millet$^{1}$, Fred van Eeuwijk$^{1}$, Bart-Jan van Rossum$^{1}$; (Maikel, Vincent, Claude, Fran\`cois, Roberto ?)
		\\
		$\ast$ Corresponding author (E-mail: willem.kruijer@wur.nl)
	\end{flushleft}


The main focus is on multi environment GWAS; multi-trait is discussed as well, but rather briefly. This is because (1) multi-trait modeling is more complicated due to the non-diagonal error structure (2) the paper will take much longer if we try to cover both (3) in multi-trait GWAS we are in my opinion not as much competitive as with multi environment gwas, at least if it comes to implementations that are currently (or soon) available. Except maybe the contrasts in the LfN paper, but there the code is poor and difficult to generalize. (4) we will stay aways from discussions on causality (see \cite{stephens_2013}), and also from competing with very high dimensional  $eQTL$ scans (which typically are rather  simplistic, from a statistical point of view, but hard to beat computationally). 

\section{Introduction}

General overview of $G \times E$, and QTL x Env mapping in experimental populations \cite{boer_etal_2007} and association panels (\cite{korte_etal_2012}, \cite{sasaki_etal_2015_multienv_gwas_Athaliana})...


Two major challenges, that we will address:
%
\begin{itemize}
\item Deriving and implementing powerful tests for detecting $QTL \times Env$ interaction. This will be the main goal of the paper.
	\item Estimating the covariance structure, and avoiding (or handling) inflation. For the previous point, it will be necessary to make a choice here, but this is less important in its own right. I do think we have potentially interesting things to say bout this, but at the same time should stay away from the rather specific and refined covariance models used in limix (and used in \cite{sasaki_etal_2015_multienv_gwas_Athaliana}, \cite{casale_etal_2015_efficientSetTests}).  
\end{itemize}
 
\section{Materials and Methods}

\subsection{Modeling and estimation of the variance-covariance}

\begin{itemize}
	\item Unstructured \cite{korte_etal_2012}, \cite{zhou_stephens_2014}
\item Unstructured, approximated pairwise \cite{furlotte_eskin_2015}
\item Unstructured, alternative approximation \cite{joo_etal_2016}
\item Factor analytic \cite{millet_etal_2016}	
\item stephens (based on beta's single env gwas): \cite{stephens_2013}
	\item limix ? Check out \verb|http://genetics.cs.ucla.edu/pylmm/| and other software at \verb|http://zarlab.cs.ucla.edu/software/|
\item low rank approximations of $K$
\end{itemize}

\subsection{Testing marker effects}

Tests:
\begin{itemize}
	\item Single-environment GWAS; then do something heuristic to combine environments (e.g. average of $-log(p)$ values)
	%
	\item The 'omnibus' test, implemented in most software: test the $H_0$ that $\beta_1 = \ldots = \beta_p = 0$ against the alternative that at least one of the $\beta_j$ is nonzero.  
%
	\item Test for a common effect: test the $H_0$ that $\beta_1 = \ldots = \beta_p = \beta = 0$ against the alternative that  $\beta \neq 0$. This was proposed by \cite{korte_etal_2012}, but implemented only for 2 traits (environments), without diagonalization. 
%
	\item Test for QTL by environment interaction: test the $H_0$ that $\beta_1 = \ldots = \beta_p = \beta$ (for some possibly nonzero $\beta$) against the alternative that not all $\beta_j$ are equal. Again, for 2 traits (environments) this was proposed by \cite{korte_etal_2012}, but with a somewhat different parameterization. Here we assume that $\beta_j = \beta + \gamma_j$, and test if $\gamma_1 = \ldots = \gamma_p = 0$.  
	See also \cite{elsoda_etal_2014}, \cite{elsoda_etal_2015}. Typically, the  test for QTL by environment interaction is only performed when the omnibus test shows at least some significance.
	
	\item Given one or more environmental covariates (ECs), test $QTL \times EC$. This has been done only for experimental populations \cite{boer_etal_2007}. For association panels it has been done in \cite{millet_etal_2016}, but only 'a posteriori', regressing $\hat \beta_1 , \ldots,  \hat \beta_p $ on e.g. temperature. Also contrasts can be dealt with (\cite{thoen_etal_2017}), in which case the EC is a vector of zeros and ones. The $QTL \times EC$ test is in fact an extension of the test for a common effect: it is now assumed that $\beta_j = \beta + EC_{j} \gamma_{EC}$, and we test if $\gamma_{EC} = 0.$
	
	%
	\item Instead of the usual regression of phenotype on genotype, it has been proposed to regress markers on traits or environments; see Garin (Msc thesis), 
	\cite{sung_etal_2016_gwas_longitudinal} and \cite{feng_2014}. What about \cite{he_etal_2013_multitrait_gwas} ?
	% 
	\item To do: Check for additional tests in limix
\end{itemize}

An important contribution of this work will be that we provide an efficient implementation for the tests for a common effect,  QTL by environment and  QTL by EC, and may be the first to do GWAS with these tests for more than 2 environments.

\subsection{Software}

Advertize our software here. We may also use GEMMA, GAMMA, TASSEL(?) and LIMIX

\section{Results}





Discuss different sources of inflation: 
\begin{itemize}
	\item in the F/Wald/score test itself \cite{zhou_stephens_2014}
	\item a strong QTL elsewhere (\cite{segura_etal_2012})
	\item epistasis (\cite{kruijer_2016}, \cite{millet_etal_2016})
	\item  more generally, an inappropriate covariance model
\end{itemize}







Simulate different scenarios (type of QTLs):
\begin{itemize}
	\item QTLs with an effect in all environments, with random effect size and sign
	\item QTLs with an effect in all environments, with a more consistent effect
	\item As in the previous two points, but only in k out of p environments
	\item QTL x EC
\end{itemize}

\textcolor{red}{$\rightarrow$ How much power do the different tests have ?} 

For inspiration, see also \cite{porter_oreilly_2017_multivariateGWAS}

\textcolor{red}{Analysis of real data}

Other issues: missing values; multiple testing correction; initial analysis of trials; the use of plot data to precisely estimate G x E, ...
 
\section{Discussion}

To which extent can the methodology be used for multi-trait data obtained from different plants. In any case, modelling multi-trait data measured on the same plants is more challenging.

Multi-locus models; \cite{millet_etal_2016}, \cite{segura_etal_2012}

\appendix

\section{Estimating marker effects}

%As in related work (see among others \cite{stephens_2013}, \cite{zhou_stephens_2014}, \cite{korte_etal_2012}) 

\subsection{The general multi-trait mixed model}

We consider the multi-trait mixed model of \cite{zhou_stephens_2014}, and adopt to a large extent their notation:
\begin{equation} \label{mtmm}
\mathbf{\tilde Y} = \mathbf{B}   \mathbf{\tilde X} + \mathbf{\tilde G} + \mathbf{\tilde E}, \quad \mathbf{\tilde G} \sim MN(0, V_g, K), \quad \mathbf{\tilde E} \sim MN(0, V_e, I_n),
\end{equation}
%
where tilde superscripts distinguish the resepctive matrices from their transformed counterparts introduced below. The $p \times n$ matrix $\mathbf{\tilde Y}$ contains phenotypic values for $p$ traits and $n$ genotypes, and the matrices $\mathbf{G}$ and $\mathbf{E}$ contain the corresponding genetic and residual effects. Given an $n \times n$ genetic relatedness matrix $K$ and $p \times p$ matrices $V_g$, $V_e$ containing the genetic and residual covariances between traits, it is assumed that $\mathbf{\tilde G}$ follows a matrix-variate normal distribution with row covariance $V_g$ and column covariance $K$; similarly, $\mathbf{\tilde E}$ is matrix-variate normal with row covariance $V_e$ and column covariance $I_n$ (the identity matrix). The $p \times c$ matrix $\mathbf{B}$ contains the trait (or environment) specific effects of $c$ covariates, whose values are contained in the $c \times n$ matrix $\mathbf{\tilde X}$.
Throughout this text we will always assume that $\mathbf{\tilde X}$ contains at least an intercept; hence $c \geq 1$. In multi-trait GWAS $\mathbf{\tilde X}$ will also contain marker scores; this is discussed in more detail in section \ref{gwas} below. 

The sum $\mathbf{\tilde G} + \mathbf{\tilde E}$ does in general not have a matrix-variate normal distribution, but we do have that $vec(\mathbf{\tilde G} + \mathbf{\tilde E})$ is multivariate normal, where $vec$ denotes the operation of creating a column vector by stacking the columns of a matrix:
%
\begin{equation} \label{mtmmvec}
\mathbf{\tilde y} = vec(\mathbf{\tilde Y}) \sim N\left(vec(\mathbf{B}  \mathbf{\tilde X}),  K \otimes V_g  + I_n \otimes V_e\right),
\end{equation}
in which $\otimes$ denotes the Kronecker product.

\subsection{Diagonalization}

Following again \cite{zhou_stephens_2014}, we use the eigen-decomposition  $K = U_k D_k U_k^t$, for an $n \times n$ orthogonal matrix $U_k$ and $D_k$ being the diagonal matrix with eigenvalues $\delta_1,\ldots,\delta_n$. If we post-multiply all terms in \eqref{mtmm} with $U_k$ and stack the columns, we obtain  
%
\begin{equation} \label{mtmmvec2}
\mathbf{y} = vec(\mathbf{Y}) = vec(\mathbf{\tilde Y} U_k) \sim N\left(vec(\mathbf{B} \mathbf{X}),  D_k \otimes V_g  + I_n \otimes V_e\right).
\end{equation}
%
where $\mathbf{X} = \mathbf{\tilde X} U_k$. Models \eqref{mtmmvec} and \eqref{mtmmvec2} do not have the same likelihood (CORRECT ?), but are equivalent in the sense that the fixed effects matrix $\mathbf{B}$ is the same. The big advantage of \eqref{mtmmvec2} is that the covariance matrix is now block-diagonal with blocks $V_i = \delta_i V_g + V_e$ ($i = 1,\ldots,n$); consequently $(D_k \otimes V_g  + I_n \otimes V_e)^{-1}$ is also block diagonal, with blocks $V_i^{-1}$. Note that this is not the case if the observations are grouped per trait (i.e. $vec(\mathbf{Y}^t)$), instead of per genotype; this is the main reason for having genotypes as rows in \eqref{mtmm}. Summarizing, after transforming $\mathbf{\tilde Y}$ and $\mathbf{\tilde X}$, inversion of the  $np \times np$ covariance matrix  only requires $n$ times an inversion of a $p \times p$ matrix.
%computation of generalized least squares (GLS) estimates

\subsection{Multi-trait GWAS: estimation of marker effects} \label{gwas}

We now take a closer look at the fixed effects, and will from this point onwards assume that $\mathbf{X}$ is of dimension $(c+1) \times n$, and of the form $\mathbf{X} = \left[\begin{array}{c}
W \\ x^t
\end{array}\right]$. %[\overset{W}{x^t}]$. 
The $p \times 1$ vector $x$ contains the marker scores of a particular SNP, and the $c \times p$ matrix $W$ the remaining covariates.
Recall that both $W$ and $x$ are the transformed versions of the original covariates; in particular $x^t = \tilde x^t U_k$, where $\tilde x$ is the original vector of marker-scores (typically containing values 0,1 and 2, but also non-integer values like probabilities or dosages are allowed).

To perform the GWAS, we re-fit model \eqref{mtmmvec2} for each SNP in turn, i.e. the vector $x$ changes from marker to marker, while $W$ stays constant. We also keep the estimates of $V_g$ and $V_e$ constant, which have been obtained for a model without SNPs (i.e., with only $W$). We do however allow  $V_g$ and $V_e$ to be refitted for each chromosome, as was proposed by \cite{rincent_etal_2014} in a univariate setting.


When $\mathbf{x}_i$ denotes the $i$th column of $\mathbf{X}$, the GLS estimates for the regression coefficients (not only the marker-effects) are given by
\begin{equation} \label{gls1}
\hat \beta_{GLS} = \left(\sum_{i=1}^n (\mathbf{x}_i \mathbf{x}_i^t) \otimes \hat V_i^{-1} \right)^{-1}
\left( \sum_{i=1}^n \mathbf{x}_i \otimes (\hat V_i^{-1} \mathbf{y}_i)\right),
%
\end{equation}
where the $p \times 1$ vector $\mathbf{y}_i$ is the $i$th column of $\mathbf{Y}$, i.e. the transformed phenotypic values of individual $i$.
The last $p$ elements in $\hat \beta_{GLS}$ correspond to the marker-effects. Note that equation (65) in \cite{zhou_stephens_2014} (supplement, p.23) seems incorrect. The variance of $\hat \beta_{GLS}$ is given by
\begin{equation*}
Var (\hat \beta_{GLS}) = \left(\sum_{i=1}^n (\mathbf{x}_i \mathbf{x}_i^t) \otimes \hat V_i^{-1} \right)^{-1}.
\end{equation*}
%Note that \cite{zhou_stephens_2014} gives an expression for the inverse of $Var (\hat \beta)$, not $Var (\hat \beta)$ itself.

%
In section \ref{test_section} below we describe the F-test for testing if the marker-effect is zero in all environments, but first we discuss numerically efficient ways to compute the effect-estimates.


\subsection{Partitioning the design matrix}
	
Equation \eqref{gls1} gives the GLS-estimate of all marker-effects as well as covariate effects. Estimates of the latter will be marker-dependent, but many of the quantities required for the estimates of the (non-marker) covariate effects are independent of the markers, and need to be evaluated only once. For this reason it is numerically more efficient if we explicitly partition $\beta$; corresponding with the different components of the design matrix. To see how this follows from well known linear algebra and classical results in regression, we will consider, just in sections \ref{ols}-\ref{gls} below, a general design matrix $X$ with the samples (individuals) corresponding to rows and variables to columns. To make the connection with the rest of this work, we have to choose $X = \mathbf{X}^t \otimes I_p$, where $\mathbf{X}^t = [W^t\: \:x]$ is of dimension $n \times (c+1)$. The $(\mathbf{X}^t \otimes I_p)$ form is also used in \cite{zhou_stephens_2014} (supplement; e.g. their equation (3)). We can now derive equation \eqref{gls1} from the   
standard GLS expression $\hat \beta = (X^t V^{-1} X)^{-1} X^t V^{-1} y$, using that $V$ is block diagonal with blocks $V_i$, and that therefore $X^t V^{-1} X$ and $X^t V^{-1}$ can be expressed in terms of $p \times p$ blocks. 


%$y$ is grouped per genotype, i.e first the observations on all traits for the first genotype, then on the second genotype, etc. 

\subsubsection{Classical theory for ordinary least squares (OLS)} \label{ols}
	
When, in OLS, the $n \times p$ design matrix $X$ consists of blocks   
$X_1$ and $X_2$ of dimensions $n \times p_1$ and $n \times p_2$, for some $p_1$ and $p_2$ summing to $p$, the corresponding least squares estimates are given by 

\begin{equation} \label{beta1}
\hat \beta_1 = X_S^{-1} X_1^t y   - X_S^{-1} (X_1^t X_2) (X_2^t X_2)^{-1} X_2^t y
\end{equation}
\begin{equation} \label{beta2}
\begin{split}
\hat \beta_2 = (X_2^t X_2)^{-1} X_2^t y + (X_2^t X_2)^{-1} X_2^t X_1 X_S^{-1} X_1^t X_2 (X_2^t X_2)^{-1} X_2^t y \\
\quad \quad - (X_2^t X_2)^{-1} X_2^t X_1 X_S^{-1} X_1^t  y,
\end{split}
\end{equation}
%
where 
\begin{equation}\label{XS}
X_S = X_1^t X_1 - X_1^t X_2 (X_2^t X_2)^{-1} X_2^t X_1
\end{equation}
is the well-known Schur-complement. These expressions can be derived by working out the block inverse $(X^t X)^{-1} = ([X_1 \: X_2]^t [X_1 \: X_2])^{-1}$ in
%
\begin{equation} \label{beta}
\hat \beta = (X^t X)^{-1} X^t y = \left[\begin{array}{c}
\hat \beta_1 \\
\hat \beta_2
\end{array}
\right],
\end{equation}
see e.g. \verb|https://www.le.ac.uk/users/dsgp1/COURSES/LEIMETZ/PARTIMOD.pdf|.
For the case $p_2=1$, see \cite{jones_1970_recursive}, or, in the matrix cookbook (\cite{petersen2008matrix}), section 3.2.6 ("Rank-1 update of inverse of inner product"). Also: include related references on the Frisch-Waugh-Lovell theorem.

Another way to obtain \eqref{beta1}-\eqref{beta2} is to define the projection matrix $P_2 = I - X_2 (X_2^t X_2)^{-1} X_2^t$ and solve the least-squares problem $P_2 y = (P_2 X_1) \beta_1 + e$. Alternatively, we can solve $P_1 y = (P_1 X_2) \beta_2 + e$ (which gives a different $X_S$). 

\subsubsection{Extension to GLS} \label{gls}

The same can be done for GLS: when $y \sim N(X \beta, V)$, then 
\begin{equation} \label{gls2}
\hat \beta_1 = X_S^{-1} X_1^t V^{-1} y   - X_S^{-1} (X_1^t V^{-1} X_2) (X_2^t V^{-1} X_2)^{-1} X_2^t V^{-1} y
\end{equation}
\begin{equation}\label{gls3}
\begin{split}
\hat \beta_2 & = (X_2^t V^{-1} X_2)^{-1} X_2^t V^{-1} y \\
&\qquad  + (X_2^t V^{-1} X_2)^{-1} (X_2^t V^{-1} X_1) X_S^{-1} (X_1^t V^{-1} X_2) (X_2^t V^{-1} X_2)^{-1} X_2^t V^{-1} y \\
&\qquad  - (X_2^t V^{-1} X_2)^{-1} (X_2^t V^{-1} X_1) X_S^{-1} X_1^t V^{-1} y,
\end{split}
\end{equation}
%
where 
\begin{equation} \label{gls4}
X_S = X_1^t V^{-1} X_1 - X_1^t V^{-1} X_2 (X_2^t V^{-1} X_2)^{-1} X_2^t V^{-1} X_1.
\end{equation}

\subsubsection{Applying the general GLS partitioning to \eqref{gls1}}

We will now apply \eqref{gls2}-\eqref{gls4} to multi-trait GWAS, by setting $X_1 = W^t \otimes I_p$ and $X_2 = x \otimes I_p$. In each of the following equations, the left-hand side describes a matrix or vector in the notation in the R-code; after the first equality sign we express it in the generic notation of sections \ref{ols}-\ref{gls} with $X_1$, $X_2$, and $y = \mathbf{y} = vec(\mathbf{Y})$, and on the right-hand-side we work that out in terms of kronecker products involving $W$ and $x$, as in \cite{zhou_stephens_2014}. Let the $c \times 1$ column vector $w_i$ denote the $i$th column of $W$ and $x_i$ the $i$th element of $x$, and 
recall that the covariance matrix $V = (D_k \otimes V_g  + I_n \otimes V_e)$ is block diagonal with blocks
$V_i = \delta_i V_g + V_e$ ($i = 1,\ldots,n$). We have:
%
\begin{equation} \label{v}
\verb|v| = X_1^t V^{-1} \mathbf{y} = 
\sum_{i=1}^n w_i  \otimes (V_i^{-1} \mathbf{y_i})
\end{equation}
%
\begin{equation} \label{Vbeta}
\verb|Vbeta| = X_1^t V^{-1} X_1 = \sum_{i=1}^n (w_i w_i^t)  \otimes V_i^{-1}
\end{equation}
%
\begin{equation} \label{v.snp}
\verb|v.snp| = X_2^t V^{-1} \mathbf{y} = \sum_{i=1}^n x_i (V_i^{-1} \mathbf{y_i})
\end{equation}
%
\begin{equation} \label{Vbeta.snp}
\verb|Vbeta.snp| = X_2^t V^{-1} X_2 = \sum_{i=1}^n x_i^2 \: V_i^{-1}
\end{equation}
%
\begin{equation} \label{X2VinvX1}
\verb|X2VinvX1| = X_2^t V^{-1} X_1 = \sum_{i=1}^n (x_i w_i^t)  \otimes V_i^{-1} 
\end{equation}
%
In a multi-trait GWAS the last 3 quantities need to be evaluated for each SNP (change the name \verb|X2VinvX1| to \verb|X2VinvX1.snp| ?), while \verb|v| and \verb|Vbeta| need to be computed only once.

\subsubsection{Evaluating terms per individual}

In the computation of \verb|v.snp|, \verb|Vbeta.snp| and \verb|X2VinvX1.snp| we want to avoid a loop over all SNPs. Instead we loop over the individuals, and for each $i$, the corresponding terms in equations \eqref{v.snp}-\eqref{X2VinvX1} are evaluated for all SNPs simultaneously, using the \verb|tcrossprod| function. When the number of individuals and/or SNPs is large, this may only be feasible for blocks of SNPs (because of memory contraints), but even then this strategy still seems faster.

Once all quantities in equations \eqref{v}-\eqref{X2VinvX1} have been computed, the remaining calculations are those in equations \eqref{gls2}-\eqref{gls4}, and these are done in a loop over the SNPs. For each SNP, the effect-estimates for the $p$ environments are given by $\hat \beta_2$ in \eqref{gls3}, and the covariate effects by $\hat \beta_1$ in \eqref{gls2}. The latter are only needed if fitted-values and p-values are required; note that by itself, $\hat \beta_2$ can be evaulated without evalualating  $\hat \beta_1$. The SNP and covariate effects are stored in matrices \verb|EFFECTS| and \verb|EFFECTS.cov|, respectively.

For the standard errors and p-values, we also require for each SNP the matrix
%
\begin{equation} \label{Q.matrix.inv}
\verb|Q.matrix.inv| = 
\left(
\begin{array}{cc}
\verb|Vbeta| & \verb|X2VinvX1| \\
(\verb|X2VinvX1|)^t & \verb|Vbeta.snp|
\end{array}
\right)^{-1}
=
\left(
\begin{array}{cc}
X_1^t V^{-1} X_1 & X_2^t V^{-1} X_1 \\
X_1^t V^{-1} X_2 & X_2^t V^{-1} X_2
\end{array}
\right)^{-1}.
\end{equation}
%
The standard errors of covariate and SNP effects (in that order) are given by the square root of the diagonal of this matrix. During the compuation of the SNP-effects, we store the standard errors of the SNP-effects in \verb|EFFECTS.se|. For later use we also store, in vectorized form, the matrix \verb|Q.matrix.inv|. A $((c+1)p)^2 \times n_s$ matrix \verb|Q.matrix.inv.all| contains \verb|Q.matrix.inv| for all SNPs $j=1,\ldots,n_s$.

\subsubsection{The likelihood ratio test (LRT), the F-test and p-values} \label{test_section}

Apart from the standard errors for all effects, it is usually of interest to test various statistical hypotheses for each SNP. The most common null-hypothesis is that all effects are zero ($\beta_1=\ldots = \beta_p=0$) which is tested against the alternative that at least on of the effects is nonzero.  As discussed in \cite{zhou_stephens_2014}, this hypothesis can be tested using the likelihood ratio test (LRT), the Wald test and the score test. Since \cite{zhou_stephens_2014} observed that the latter two usually give inflated respectively deflated p-values, we restrict our attention here to the LRT.

The LRT is defined as twice the difference between the log-likelihood of model \eqref{mtmmvec2} with $\mathbf{B} \mathbf{X}$ containing both the SNPs and the covariates, and the model with only the covariates. Since we use the same $V_g$ and $V_e$ for both models (and also for all SNPs), the (log) determinants of the corresponding Gaussian distributions are the same and cancel out, and the LRT is the difference between the (generalized) sums of squares of the two models, which are quadratic forms of the form
%
\begin{equation} \label{SS}
(\mathbf{y} - \mathbf{\hat  y})^t \left(D_k \otimes V_g  + I_n \otimes V_e\right)^{-1} (\mathbf{y} - \mathbf{\hat  y}),
\end{equation}
%  
where we recall that $\mathbf{y} = vec(\mathbf{Y})$, and $\mathbf{\hat  y}$ is the vector of fitted values. 
For the full model, the latter depends on the SNP as well as covariate effects. Using the classical result of ... (add reference), \cite{zhou_stephens_2014} (supplement, p. 19, equ. (26)-(29)) re-expressed \eqref{SS} as
\begin{equation} \label{SS2}
\mathbf{y}^t P \mathbf{y},
\end{equation}
where $P$ is defined in their equation (29). See also  \cite{johnson_thompson_1995}. 

Note that we use the ML-based LRT; for the REML-based LRT we get the extra terms of equation (27) in \cite{zhou_stephens_2014}.

Instead of the (ML) LRT, we use the related F-test
%
\begin{equation} \label{Ftest}
F = \frac{df_{full}}{df_{red} - df_{full}} \left(\frac{SS_{red} - SS_{full}}{SS_{full}}\right) = \frac{(n-c-1)p}{p} \left(\frac{SS_{red} - SS_{full}}{SS_{full}}\right),
\end{equation}
%
which, under the null-hypothesis, has an F-distribution with degrees of freedom $df1 = df_{red} - df_{full} = p$ and $df2 = df_{full} = (n-c-1)p$.

To compute $F$ we need $SS_{red}$ and $SS_{full}$, i.e. equations \eqref{SS} and \eqref{SS2} for the reduced and the full model. The former is the same for all SNPs, and we evaluate it with our \verb|LL.quad.form.diag| function. It would be convenient to use the same for $SS_{full}$, but this would require storing all $np$ residuals for all $n_s$ SNPs. To avoid this we use equation (50) of \cite{zhou_stephens_2014}, on p.21: $\mathbf{y}^t P \mathbf{y} = q - \mathbf{q}^t \mathbf{Q^{-1}} \mathbf{q}$, where the terms on the right-hand-side are define at the bottom of p.20. This formula is also used in the \verb|LL.quad.form.diag| function. However, instead of directly looping over all SNPs and calling \verb|LL.quad.form.diag| each time, we first compute the terms $\mathbf{Q^{-1}}$,  $\mathbf{q}$ and $q$ for all SNPs simultaneously, looping over the individuals.   

\section{Simulation}

Suppose we have variance-covariance matrices $S \in \R^{n \times n}$ and $T \in \R^{p \times p}$.
When $Z$ is an $n \times p$ matrix whose elements follow independent standard normal distributions, $Y = S^{\frac{1}{2}} Z T^{\frac{1}{2}}$ follows
the matrix-variate normal distribution $MN(M; S, T)$ (\verb|https://stat.ethz.ch/pipermail/r-help/2012-February/302442.html|; find a reference).

\subsection{R-code}

%\begin{lstlisting}
\begin{verbatim}
# http://realizationsinbiostatistics.blogspot.com/2008/08/matrix-square-roots-in-r_18.html
MatrixRoot <- function(x) { # assumes that x is symmetric
x.eig <- eigen(x,symmetric=TRUE)
x.sqrt <- x.eig$vectors %*% diag(sqrt(x.eig$values)) %*% solve(x.eig$vectors)
return(x.sqrt)
}
\end{verbatim}
%\end{lstlisting}


\bibliography{M:/willem/research/statgen4}

\end{document} 